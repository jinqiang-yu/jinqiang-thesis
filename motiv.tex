\section{Motivations of Formal Explainability} \label{sec:motiv}

While model-agnostic methods are prevalent in the XAI domain, 
they encounter challenges regarding the quality of explanations. 
%
An emerging alternative is formal explainability, which represents a
rising trend in applying automated reasoning techniques to explain and 
verify ML models.
%
The challenges faced by model-agnostic methods 
and the emergence of formal explainability 
are outlined below.

%\subsection{Limitations of Model-agnostic Explanations}

\textbf{Limitations of Model-agnostic Explanations}.
Numerous XAI techniques introduced in recent years are 
model-agnostic, yet they unfortunately encounter several issues.
%
These issues include out-of-distribution sampling~\cite{lakkaraju-aies20a,lb-aies20,yis-corr23,yfis-corr23}
and unsoundness in generated explanations~\cite{inms-corr19,nsmims-sat19,ignatiev-ijcai20,msi-aaai22}.
%
Due to the limitations presented by these non-formal XAI methods,
a notable challenge regarding the reliability of model-agnostic explanations is identified, 
particularly in high-risk or safety-critical environments~\cite{rudin-nature22,rudin-natmi19,vw-pt21,raai-sp19,dmbt-comp17,hcw-chb21,ms-rw22}.
%
Relying on such unsound explanations generated by model-agnostic methods can trigger severe consequences.
%
In addition to unsoundness, other drawbacks of model-agnostic explanations have been identified~\cite{cgflb-corr19,dbjw-ecai20,kvsf-icml20,jmb-aistats20,hms-corr23,hms-corr23b,hms-corr23c}.

\textbf{Formal Explainability}.
%
Formal XAI~(FXAI) approaches, providing \emph{logic-based explanations} or 
\emph{formal explanations},
stand as an alternative to  model-agnostic methods.
%
These formal XAI methods are characterized by \newm{model-specific}
and logic-based XAI methods~\cite{ignatiev-ijcai20,msi-aaai22,darwiche-lics23,ms-rw22}.
%
FXAI is aimed at offering rigorous and provable explanations for 
outputs made by ML models.
%
There are two categories of formal explanations, namely,
\emph{abductive explanations}~(AXp’s) and \emph{contrastive explanations}~(CXp’s).
%
AXp's provide answers for \emph{why} a prediction was made, 
while CXp's answer questions of \emph{why not} another prediction was made
or \emph{how} to change the prediction produced by the ML model. 
%
Several studies introduce formal methods for computing explanations~\cite{amgoud-ecsqaru21,ll-clar21,rcblt-ai21,rcbt-kr20,wgh-aies19}, with one prominent
approach in formal explainability building on abductive reasoning~\cite{inms-corr19,inms-aaai19,ignatiev-ijcai20,msgcin-nips20,ims-sat21,msi-aaai22,imsns-ijcai21,hiicams-aaai22,barcelo-nips22,hiims-kr21,hims-aaai23,huang-tacas23,ims-ijcai21,msgcin-icml21,yisnms-aaai23,snimmsv-aaai22,nsmims-sat19}.

\textbf{Abductive Reasoning for Formal Explanations}.
%
In general, abduction-based approaches involves encoding a given
ML model into a logical-based representation.
%
Therefore, the efficiency of generating explanations through
these methods relies on optimizing the encoding \newm{of} ML models 
to logic-based representations, along with the capabilities of automated 
reasoning tools, e.g. SAT~(Boolean satisfiability), SMT~(satisfiability modulo theories), 
and QBF~(quantified Boolean formula) solvers.
%
%However, recent research has presented a technique that uses diverse 
%robustness tools to generate formal explanations~\cite{hms-corr23d}, 
%which does not explicitly demand the encoding \newm{of} ML models 
%to logic-based representations.

\textbf{Explainability for Interpretable Models}.
%
Intrinsically interpretable ML models~\cite{molnar-bk20,rudin-nmi-19,blrs-ceur22,lzlsr-neurips22,clrsww-dss22,srp-fat22}, 
as their name implies, can offer explanations without requiring extra computation.
%
Decision trees~\cite{rivest-ipl76,breiman-bk84,quinlan-bk93,quinlan-ml86}, 
decision lists~\cite{rudin-mpc18,rivest-ml87}, 
and decision sets~\cite{leskovec-kdd16,michalski-isip69} are widely considered as 
the most interpretable ML models.
%
Recent studies have revealed that formal explanations for decision trees 
can be notably more compact than explanations offered by interpretable ML models~\cite{iims-corr20,iims-jair22},
with decision tree explanations aligning with the paths within the tree.
%
Moreover, the interpretability of decision lists presents challenges~\cite{msi-fai23}. 
%and sets 
%
These findings indicate that even interpretable models still require explanation or 
further interpretability improvement.
