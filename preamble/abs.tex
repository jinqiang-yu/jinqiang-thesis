% https://www.monash.edu/graduate-research/examination/publication
% https://www.monash.edu/rlo/graduate-research-writing/write-the-thesis
% https://www.monash.edu/rlo/graduate-research-writing/write-the-thesis/writing-the-thesis-chapters/structuring-a-long-text
%\abstract{
%\addtocontents{toc}{}  % Add a gap in the Contents, for aesthetics

\section*{Abstract}
\addtotoc{Abstract}%Copyright notice}

In recent years, machine learning~(ML) and Artificial Intelligence~(AI) techniques have
experienced rapid advancements, reshaping numerous aspects of human lives.
%
Owing to advancements in algorithms and improved computational capabilities, 
%and the availability of vast datasets,
these impressive ML and AI approaches are adopted across a wide range of tasks, 
including those in the fields of natural language processing~(NLP) and computer~vision (CV).
%
For instance, a significant breakthrough has been achieved with the development of 
large language models (LLMs) such as GPT-4, which have demonstrated exceptional 
performance in a variety of natural language processing~(NLP) tasks.

Despite the considerable advancements in machine learning (ML) that 
have rapidly integrate complex ML models into our daily lives, 
they are considered ``black-box'' models, lacking transparency in the outputs they produce. 
%
Consequently, both domain experts and general users lack 
clear insights into outputs generated by these complex models, 
although they make decisions based on these outputs.
%
As a result, there is a growing demand for transparency and accountability in 
decision-making processes since lacking them can lead to critical issues related to fairness and bias, 
as well as regulatory compliance. 
%
This arouses the need for rapid development in the research field of \emph{eXplainable AI}~(XAI).
%
The aim of XAI is to connect the internal workings of ML/AI systems with human understanding
and ultimately establish trustworthy AI.
%
Various benefits of XAI are identified, including reducing bias in ML models,
building trust in AI systems, and enhancing safety in safety-critical domains.

A wide variety of XAI methods towards enhancing transparency and trustworthiness of ML systems
have emerged in recent years.
%
Generally, XAI techniques can be classified according to various criteria, including intrinsic, 
post-hoc, model-agnostic, and model-specific methods.
%
Among these techniques, model-agnostic methods are prominent in the XAI field,
which produce two types of explanations, namely, feature selection and
feature attribution explanations.
%
A feature selection explanation identifies a set of features sufficient to get the prediction, 
whereas a feature attribution explanation indicates the importance of each feature for the prediction.
%
However, model-agnostic methods face explanation quality challenges, such as issues with 
out-of-distribution sampling and unsoundness in the produced explanations.
%
The limitations of these non-formal XAI approaches trigger a significant challenge 
to the reliability of model-agnostic explanations, especially in high-risk or 
safety-critical settings.
%
Consequently, serious outcomes may arise from relying on such unsound
explanations produced by model-agnostic methods.

As an alternative, formal eXplainable AI~(FXAI) methods offer logic-based or formal explanations,
aiming to provide rigorous and provable explanations for predictions produced by ML models.
%
These formal XAI techniques are characterized by logic-based and model-specific XAI approaches,
where explanations generated by formal methods are classified as two categories, namely, abductive explanations (AXp's) and
contrastive explanations (CXp's).
%
AXP's provide answers for ``'why'' questions, i.e. why a certain prediction was made, 
while CXP's address ``why'' nor ``how'' questions, i.e. why not another prediction was made or
how to change the prediction.
%
The thesis is motivated by rapid advancements of the XAI field and the shortcomings 
of prevailing model-agnostic approaches, focusing on formal explainability
and aiming to tackle the challenges in model-agnostic methods as well
as enhance formal explainability.
%
Specifically, the thesis introduces a method to compute decision sets and lists that optimize either 
the number of literals used in the model or the trade-off between model size and accuracy.
%
This addresses the explainability challenges encountered in interpretable models 
produced by prior studies.
%
The thesis also propose an innovative anytime method for producing decision sets
that are both accurate and interpretable.
%
This involves compiling a gradient boosted tree into a decision set, 
resolving accuracy concerns associated with decision sets.
%
Second, the thesis presents an approach to generating more succinct AXp's and more accurate CXp's 
with the use of background knowledge, improving the quality of AXp's and CXp's generated by
existing approaches.
%
Furthermore, the thesis introduces the first method to produce formal feature attribution 
and extends the approach to more efficiently producing or approximating formal feature
attribution.
%
Finally, a method to apply formal explainability in just-in-time (JIT) defect prediction is proposed.
%
The proposed approach can efficiently generate explanations that are demonstrated
to be accurate, robust, and actionable, whereas lack of this capability is 
identified in existing model-agnostic techniques.






%The abstract should outline the main approach and findings of the thesis and must not be more than 500 words.
% 
%The Thesis Abstract is written here (and usually kept to just this page). The page is kept centered vertically so can expand into the blank space above the title too\ldots

%}
